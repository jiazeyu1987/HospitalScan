# æ‹›æŠ•æ ‡ä¿¡æ¯ç›‘æ§çˆ¬è™«ç³»ç»Ÿè®¾è®¡æ–‡æ¡£

**ä½œè€…ï¼š** MiniMax Agent  
**ç‰ˆæœ¬ï¼š** v1.0  
**æ—¥æœŸï¼š** 2025-11-18  
**é¡¹ç›®ï¼š** å…¨å›½åŒ»é™¢å®˜ç½‘æ‰«æä¸æ‹›æŠ•æ ‡ç›‘æ§ç³»ç»Ÿ

---

## ğŸ¯ ä¸€ã€ç³»ç»Ÿæ¦‚è¿°

### 1.1 è®¾è®¡ç›®æ ‡
æ‹›æŠ•æ ‡ä¿¡æ¯ç›‘æ§çˆ¬è™«ç³»ç»Ÿæ—¨åœ¨è‡ªåŠ¨å‘ç°ã€è§£æå’Œç›‘æ§åŒ»é™¢å®˜ç½‘ä¸­çš„æ‹›æŠ•æ ‡ç›¸å…³ä¿¡æ¯ï¼Œé€šè¿‡æ™ºèƒ½åŒ–çš„æ ç›®è¯†åˆ«ã€å†…å®¹æå–å’Œå»é‡æœºåˆ¶ï¼Œå®ç°æ‹›æŠ•æ ‡ä¿¡æ¯çš„å®æ—¶ç›‘æ§å’Œæ›´æ–°ã€‚

### 1.2 æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

```python
# ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ
TenderMonitoringSystem:
â”œâ”€â”€ SectionDetector      # æ‹›æŠ•æ ‡æ ç›®è‡ªåŠ¨è¯†åˆ«å™¨
â”œâ”€â”€ ContentExtractor     # å†…å®¹è§£ææå–å™¨
â”œâ”€â”€ DateExtractor       # æ—¥æœŸä¿¡æ¯æå–å™¨
â”œâ”€â”€ ContentHasher       # å†…å®¹å“ˆå¸Œå»é‡å™¨
â”œâ”€â”€ ChangeDetector      # å¢é‡æ›´æ–°æ£€æµ‹å™¨
â”œâ”€â”€ DataProcessor       # æ•°æ®å¤„ç†æ ‡å‡†åŒ–
â”œâ”€â”€ QualityValidator    # æ•°æ®è´¨é‡éªŒè¯å™¨
â””â”€â”€ NotificationManager # é€šçŸ¥ç®¡ç†å™¨
```

### 1.3 æŠ€æœ¯æµç¨‹

```
å®˜ç½‘è®¿é—® â†’ æ ç›®å‘ç° â†’ å†…å®¹æŠ“å– â†’ ä¿¡æ¯æå– â†’ å»é‡éªŒè¯ â†’ è´¨é‡æ£€æŸ¥ â†’ æ•°æ®å­˜å‚¨ â†’ å˜æ›´é€šçŸ¥
```

---

## ğŸ” äºŒã€æ‹›æŠ•æ ‡æ ç›®è‡ªåŠ¨è¯†åˆ«ç®—æ³•

### 2.1 æ ç›®è¯†åˆ«ç­–ç•¥

```python
class SectionDetector:
    """æ‹›æŠ•æ ‡æ ç›®è‡ªåŠ¨è¯†åˆ«å™¨"""
    
    def __init__(self):
        self.tender_keywords = self._load_tender_keywords()
        self.section_patterns = self._load_section_patterns()
        self.confidence_threshold = 0.6
    
    def detect_tender_sections(self, soup: BeautifulSoup, url: str) -> List[TenderSection]:
        """æ£€æµ‹æ‹›æŠ•æ ‡ç›¸å…³æ ç›®"""
        sections = []
        
        # 1. å¯¼èˆªèœå•æ£€æµ‹
        nav_sections = self._detect_in_navigation(soup)
        sections.extend(nav_sections)
        
        # 2. é¡µé¢å†…å®¹æ£€æµ‹
        content_sections = self._detect_in_content(soup)
        sections.extend(content_sections)
        
        # 3. é¡µé¢åº•éƒ¨é“¾æ¥æ£€æµ‹
        footer_sections = self._detect_in_footer(soup)
        sections.extend(footer_sections)
        
        # 4. é¡µé¢URLæ¨¡å¼æ£€æµ‹
        url_sections = self._detect_by_url_pattern(url)
        sections.extend(url_sections)
        
        # 5. è¯„åˆ†å’Œå»é‡
        scored_sections = self._score_sections(sections)
        filtered_sections = self._filter_by_confidence(scored_sections)
        
        return filtered_sections
    
    def _detect_in_navigation(self, soup: BeautifulSoup) -> List[TenderSection]:
        """ä»å¯¼èˆªèœå•ä¸­æ£€æµ‹æ‹›æŠ•æ ‡æ ç›®"""
        sections = []
        
        # æŸ¥æ‰¾å¯¼èˆªèœå•
        nav_selectors = [
            'nav', '.nav', '#nav', '.navigation', 
            '.menu', '.main-menu', '.header-menu'
        ]
        
        for selector in nav_selectors:
            nav_elements = soup.select(selector)
            for nav in nav_elements:
                links = nav.find_all('a', href=True)
                for link in links:
                    section_info = self._analyze_link_for_tender(link)
                    if section_info:
                        sections.append(section_info)
        
        return sections
    
    def _detect_in_content(self, soup: BeautifulSoup) -> List[TenderSection]:
        """ä»é¡µé¢å†…å®¹ä¸­æ£€æµ‹æ‹›æŠ•æ ‡æ ç›®"""
        sections = []
        
        # æŸ¥æ‰¾åŒ…å«æ‹›æŠ•æ ‡å…³é”®è¯çš„å†…å®¹åŒºåŸŸ
        content_keywords = [
            'æ‹›æ ‡', 'é‡‡è´­', 'å…¬ç¤º', 'å…¬å‘Š', 'ä¸­æ ‡', 
            'æŠ•æ ‡', 'ç«æ ‡', 'æ‹›æ ‡é‡‡è´­', 'ä¿¡æ¯å‘å¸ƒ'
        ]
        
        for keyword in content_keywords:
            # ä½¿ç”¨å…¨æ–‡æœç´¢æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„å…ƒç´ 
            elements = soup.find_all(text=lambda text: text and keyword in text)
            
            for element in elements:
                parent = element.parent
                if parent and parent.name in ['a', 'div', 'span']:
                    section_info = self._analyze_element_for_tender(parent, keyword)
                    if section_info:
                        sections.append(section_info)
        
        return sections
    
    def _detect_by_url_pattern(self, url: str) -> List[TenderSection]:
        """æ ¹æ®URLæ¨¡å¼æ£€æµ‹æ‹›æŠ•æ ‡æ ç›®"""
        sections = []
        
        # URLä¸­åŒ…å«æ‹›æŠ•æ ‡ç›¸å…³å…³é”®è¯
        url_tender_patterns = [
            (r'/zb/', 'æ‹›æ ‡'),
            (r'/cg/', 'é‡‡è´­'),
            (r'/gs/', 'å…¬ç¤º'),
            (r'/gg/', 'å…¬å‘Š'),
            (r'/zbgg/', 'æ‹›æ ‡å…¬å‘Š'),
            (r'/zbgs/', 'æ‹›æ ‡å…¬ç¤º'),
            (r'/tendermaster/', 'æ‹›æŠ•æ ‡'),
            (r'/procurement/', 'é‡‡è´­'),
            (r'/bidding/', 'æŠ•æ ‡')
        ]
        
        for pattern, description in url_tender_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                sections.append(TenderSection(
                    name=description,
                    url=url,
                    type='url_pattern_match',
                    confidence=0.8
                ))
        
        return sections
    
    def _load_tender_keywords(self) -> Dict:
        """åŠ è½½æ‹›æŠ•æ ‡å…³é”®è¯åº“"""
        return {
            'main': [
                'æ‹›æ ‡', 'é‡‡è´­', 'æ‹›æŠ•æ ‡', 'æ‹›æ ‡é‡‡è´­', 'æ‹›é‡‡',
                'ä¸­æ ‡', 'ä¸­æ ‡ç»“æœ', 'ä¸­æ ‡å…¬å‘Š', 'ä¸­æ ‡å…¬ç¤º',
                'æŠ•æ ‡', 'ç«æ ‡', 'æ‹›æ ‡å…¬å‘Š', 'æ‹›æ ‡å…¬ç¤º',
                'é‡‡è´­å…¬å‘Š', 'é‡‡è´­å…¬ç¤º', 'é‡‡è´­ç»“æœ'
            ],
            'secondary': [
                'å…¬å‘Š', 'å…¬ç¤º', 'é€šçŸ¥', 'ä¿¡æ¯å‘å¸ƒ',
                'é‡‡è´­ä¿¡æ¯', 'æ‹›æ ‡ä¿¡æ¯', 'é‡‡è´­é¡¹ç›®',
                'å·¥ç¨‹æ‹›æ ‡', 'è®¾å¤‡é‡‡è´­', 'æœåŠ¡é‡‡è´­',
                'è¯å“é‡‡è´­', 'è€—æé‡‡è´­', 'è¯•å‰‚é‡‡è´­'
            ],
            'url_patterns': [
                (r'/zb/?', 0.4),
                (r'/cg/?', 0.4),
                (r'/gg/?', 0.3),
                (r'/gs/?', 0.3),
                (r'/tender', 0.5),
                (r'/bidding', 0.5),
                (r'/procurement', 0.5),
                (r'/announcement', 0.3)
            ]
        }

@dataclass
class TenderSection:
    """æ‹›æŠ•æ ‡æ ç›®ä¿¡æ¯"""
    name: str
    url: str
    type: str  # 'navigation', 'content', 'footer', 'url_pattern'
    confidence: float
    category: Optional[str] = None
    description: str = ""
```

---

## ğŸ“„ ä¸‰ã€HTMLå†…å®¹è§£æå’Œç»“æ„åŒ–æå–

### 3.1 å†…å®¹æå–å™¨

```python
class ContentExtractor:
    """å†…å®¹æå–å™¨"""
    
    def __init__(self):
        self.parsers = {
            'list_page': ListPageParser(),
            'detail_page': DetailPageParser(),
            'table_page': TablePageParser()
        }
        self.content_cleaner = ContentCleaner()
    
    def extract_tender_info(self, url: str, soup: BeautifulSoup, 
                          section_info: TenderSection) -> List[TenderRecord]:
        """æå–æ‹›æŠ•æ ‡ä¿¡æ¯"""
        # 1. æ£€æµ‹é¡µé¢ç±»å‹
        page_type = self._detect_page_type(soup, url)
        
        # 2. é€‰æ‹©åˆé€‚çš„è§£æå™¨
        parser = self.parsers.get(page_type, self.parsers['list_page'])
        
        # 3. æå–å†…å®¹
        raw_records = parser.parse(soup, url, section_info)
        
        # 4. æ¸…ç†å’Œæ ‡å‡†åŒ–
        cleaned_records = []
        for record in raw_records:
            cleaned_record = self.content_cleaner.clean_tender_record(record)
            if cleaned_record:
                cleaned_records.append(cleaned_record)
        
        return cleaned_records

class ListPageParser:
    """åˆ—è¡¨é¡µé¢è§£æå™¨"""
    
    def parse(self, soup: BeautifulSoup, url: str, section_info: TenderSection) -> List[RawTenderRecord]:
        """è§£æåˆ—è¡¨é¡µé¢"""
        records = []
        
        # 1. å°è¯•ä»è¡¨æ ¼ä¸­æå–
        table_records = self._extract_from_table(soup, url)
        records.extend(table_records)
        
        # 2. å°è¯•ä»åˆ—è¡¨é¡¹ä¸­æå–
        list_records = self._extract_from_list(soup, url)
        records.extend(list_records)
        
        # 3. å°è¯•ä»å¡ç‰‡ä¸­æå–
        card_records = self._extract_from_cards(soup, url)
        records.extend(card_records)
        
        return records
    
    def _extract_from_table(self, soup: BeautifulSoup, url: str) -> List[RawTenderRecord]:
        """ä»è¡¨æ ¼ä¸­æå–"""
        records = []
        tables = soup.find_all('table')
        
        for table in tables:
            rows = table.find_all('tr')[1:]  # è·³è¿‡è¡¨å¤´
            
            for row in rows:
                record = self._parse_table_row(row, url)
                if record:
                    records.append(record)
        
        return records
    
    def _parse_table_row(self, row, url: str) -> Optional[RawTenderRecord]:
        """è§£æè¡¨æ ¼è¡Œ"""
        cells = row.find_all(['td', 'th'])
        if len(cells) < 2:
            return None
        
        # æå–æ ‡é¢˜ï¼ˆé€šå¸¸åœ¨ç¬¬ä¸€åˆ—ï¼‰
        title_cell = cells[0]
        title_link = title_cell.find('a')
        
        if not title_link:
            return None
        
        title = title_link.get_text().strip()
        detail_url = self._resolve_url(title_link.get('href'), url)
        
        # æå–æ—¥æœŸï¼ˆé€šå¸¸åœ¨æœ€åä¸€åˆ—æˆ–ç‰¹å®šçš„åˆ—ï¼‰
        date = self._extract_date_from_row(cells)
        
        # æå–å…¶ä»–ä¿¡æ¯
        budget = self._extract_budget_from_row(cells)
        
        return RawTenderRecord(
            title=title,
            url=detail_url,
            publish_date=date,
            budget=budget,
            source_url=url,
            extraction_method='table_row'
        )

@dataclass
class RawTenderRecord:
    """åŸå§‹æ‹›æŠ•æ ‡è®°å½•"""
    title: str
    url: str
    publish_date: Optional[date] = None
    deadline_date: Optional[date] = None
    content: str = ""
    budget: Optional[float] = None
    source_url: str = ""
    extraction_method: str = ""

@dataclass
class TenderRecord:
    """æ ‡å‡†åŒ–æ‹›æŠ•æ ‡è®°å½•"""
    title: str
    url: str
    publish_date: Optional[date] = None
    deadline_date: Optional[date] = None
    content: str = ""
    budget: Optional[float] = None
    tender_type: str = "other"
    tender_category: str = "other"
    source_url: str = ""
    content_hash: str = ""
    hospital_id: Optional[int] = None
    status: str = "published"
```

---

## ğŸ”„ å››ã€Content Hashå»é‡æœºåˆ¶

### 4.1 å»é‡ç­–ç•¥è®¾è®¡

```python
class ContentHasher:
    """å†…å®¹å“ˆå¸Œå»é‡å™¨"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.hash_cache = {}
        self.similarity_threshold = 0.9
    
    def is_duplicate(self, record: TenderRecord) -> DuplicateCheckResult:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤è®°å½•"""
        result = DuplicateCheckResult(is_duplicate=False)
        
        # 1. ç²¾ç¡®å“ˆå¸ŒåŒ¹é…
        exact_match = self._check_exact_hash_match(record.content_hash)
        if exact_match:
            result.is_duplicate = True
            result.duplicate_type = 'exact_hash'
            result.similar_record = exact_match
            return result
        
        # 2. ç›¸ä¼¼åº¦æ£€æŸ¥
        similar_records = self._find_similar_records(record)
        
        # 3. URLé‡å¤æ£€æŸ¥
        url_duplicates = self._check_url_duplicate(record.url)
        
        # 4. æ ‡é¢˜ç›¸ä¼¼åº¦æ£€æŸ¥
        title_duplicates = self._check_title_similarity(record.title)
        
        # ç»¼åˆåˆ¤æ–­
        if similar_records or url_duplicates or title_duplicates:
            result.is_duplicate = True
            result.duplicate_type = self._determine_duplicate_type(
                similar_records, url_duplicates, title_duplicates
            )
            result.similar_records = {
                'similar': similar_records,
                'url_duplicate': url_duplicates,
                'title_duplicate': title_duplicates
            }
        
        return result
    
    def _generate_content_hash(self, record: TenderRecord) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œ"""
        # ä½¿ç”¨æ ‡é¢˜+å†…å®¹ç”Ÿæˆå“ˆå¸Œ
        content_for_hash = f"{record.title}|{record.publish_date}|{record.url}"
        
        if record.content:
            content_for_hash += f"|{record.content[:500]}"  # å–å‰500å­—ç¬¦
        
        return hashlib.md5(content_for_hash.encode('utf-8')).hexdigest()

@dataclass
class DuplicateCheckResult:
    """é‡å¤æ£€æŸ¥ç»“æœ"""
    is_duplicate: bool
    duplicate_type: str = ""  # 'exact_hash', 'url', 'title', 'content'
    similar_record: Optional[TenderRecord] = None
    similar_records: Optional[Dict] = None
```

---

## ğŸ“… äº”ã€åˆ†é¡µå’Œæ—¥æœŸæå–é€»è¾‘

### 5.1 åˆ†é¡µå¤„ç†ç­–ç•¥

```python
class PaginationHandler:
    """åˆ†é¡µå¤„ç†å™¨"""
    
    def __init__(self):
        self.pagination_patterns = self._load_pagination_patterns()
    
    def detect_pagination(self, soup: BeautifulSoup, url: str) -> PaginationInfo:
        """æ£€æµ‹åˆ†é¡µä¿¡æ¯"""
        pagination_info = PaginationInfo()
        
        # 1. æŸ¥æ‰¾åˆ†é¡µå¯¼èˆª
        nav_element = self._find_pagination_nav(soup)
        if nav_element:
            pagination_info = self._parse_pagination_nav(nav_element, url)
        else:
            # 2. é€šè¿‡é“¾æ¥æ¨¡å¼æ£€æµ‹åˆ†é¡µ
            pagination_info = self._detect_by_link_patterns(soup, url)
        
        return pagination_info

@dataclass
class PaginationInfo:
    """åˆ†é¡µä¿¡æ¯"""
    base_url: str = ""
    current_page: int = 1
    total_pages: int = 1
    has_pagination: bool = False
    has_next: bool = False
    has_prev: bool = False
    next_url: Optional[str] = None
    prev_url: Optional[str] = None
    page_urls: List[str] = field(default_factory=list)
```

### 5.2 æ—¥æœŸæå–å™¨

```python
class DateExtractor:
    """æ—¥æœŸæå–å™¨"""
    
    def __init__(self):
        self.date_patterns = self._load_date_patterns()
        self.date_keywords = self._load_date_keywords()
    
    def extract_dates(self, text: str) -> ExtractedDates:
        """ä»æ–‡æœ¬ä¸­æå–æ—¥æœŸä¿¡æ¯"""
        result = ExtractedDates()
        
        # 1. å‘å¸ƒæ—¥æœŸ
        result.publish_date = self._extract_publish_date(text)
        
        # 2. æˆªæ­¢æ—¥æœŸ
        result.deadline_date = self._extract_deadline_date(text)
        
        # 3. å¼€å§‹æ—¥æœŸ
        result.start_date = self._extract_start_date(text)
        
        # 4. ç»“æŸæ—¥æœŸ
        result.end_date = self._extract_end_date(text)
        
        return result

@dataclass
class ExtractedDates:
    """æå–çš„æ—¥æœŸä¿¡æ¯"""
    publish_date: Optional[date] = None
    deadline_date: Optional[date] = None
    start_date: Optional[date] = None
    end_date: Optional[date] = None
```

---

## ğŸ”„ å…­ã€å¢é‡æ›´æ–°å’Œå†å²å¯¹æ¯”

### 6.1 å˜æ›´æ£€æµ‹å™¨

```python
class ChangeDetector:
    """å¢é‡æ›´æ–°å˜æ›´æ£€æµ‹å™¨"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.scan_history = ScanHistory()
    
    def detect_changes(self, hospital_id: int, new_records: List[TenderRecord]) -> ChangeDetectionResult:
        """æ£€æµ‹å˜æ›´"""
        result = ChangeDetectionResult(hospital_id=hospital_id)
        
        try:
            # 1. è·å–å†å²è®°å½•
            historical_records = self._get_historical_records(hospital_id)
            
            # 2. æ–°å¢è®°å½•æ£€æµ‹
            new_record_hashes = {record.content_hash for record in new_records}
            historical_hashes = {record.content_hash for record in historical_records}
            
            result.new_records = [
                record for record in new_records 
                if record.content_hash not in historical_hashes
            ]
            
            # 3. æ›´æ–°è®°å½•æ£€æµ‹
            result.updated_records = self._detect_updated_records(new_records, historical_records)
            
            # 4. åˆ é™¤è®°å½•æ£€æµ‹
            result.removed_records = self._detect_removed_records(new_record_hashes, historical_records)
            
            # 5. ç”Ÿæˆå˜æ›´ç»Ÿè®¡
            result.change_summary = self._generate_change_summary(result)
            
        except Exception as e:
            result.error = str(e)
            result.status = 'failed'
        
        return result

@dataclass
class ChangeDetectionResult:
    """å˜æ›´æ£€æµ‹ç»“æœ"""
    hospital_id: int
    new_records: List[TenderRecord] = field(default_factory=list)
    updated_records: List['UpdatedRecord'] = field(default_factory=list)
    removed_records: List[TenderRecord] = field(default_factory=list)
    change_summary: Optional['ChangeSummary'] = None
    status: str = "completed"
    error: str = ""

@dataclass
class ChangeSummary:
    """å˜æ›´æ‘˜è¦"""
    new_count: int
    updated_count: int
    removed_count: int
    total_changes: int
    change_rate: float  # å˜æ›´ç‡ 0-1
```

---

## ğŸ¯ ä¸ƒã€æ€»ç»“

æœ¬æ‹›æŠ•æ ‡ä¿¡æ¯ç›‘æ§çˆ¬è™«ç³»ç»Ÿè®¾è®¡æä¾›äº†ï¼š

### 7.1 æ ¸å¿ƒåŠŸèƒ½
1. **æ™ºèƒ½æ ç›®è¯†åˆ«** - å¤šç»´åº¦æ‹›æŠ•æ ‡æ ç›®è‡ªåŠ¨å‘ç°
2. **ç²¾å‡†å†…å®¹æå–** - åˆ—è¡¨é¡µã€è¯¦æƒ…é¡µã€è¡¨æ ¼é¡µé€‚é…è§£æ
3. **é«˜æ•ˆå»é‡æœºåˆ¶** - Content Hash + ç›¸ä¼¼åº¦ç®—æ³•åŒé‡å»é‡
4. **å‡†ç¡®æ—¥æœŸæå–** - å‘å¸ƒæ—¥æœŸã€æˆªæ­¢æ—¥æœŸç­‰å…³é”®æ—¶é—´ä¿¡æ¯æå–
5. **å¢é‡æ›´æ–°æ£€æµ‹** - æ–°å¢ã€æ›´æ–°ã€åˆ é™¤è®°å½•çš„ç²¾ç¡®è¯†åˆ«
6. **åˆ†é¡µæ™ºèƒ½å¤„ç†** - è‡ªåŠ¨è¯†åˆ«å’Œéå†åˆ†é¡µå†…å®¹

### 7.2 æŠ€æœ¯ç‰¹ç‚¹
- **è‡ªé€‚åº”è§£æ** - è‡ªåŠ¨è¯†åˆ«é¡µé¢ç»“æ„ï¼Œé€‰æ‹©åˆé€‚çš„è§£æç­–ç•¥
- **å¤šå±‚æ¬¡å»é‡** - å“ˆå¸ŒåŒ¹é… + URLé‡å¤ + æ ‡é¢˜ç›¸ä¼¼åº¦æ£€æŸ¥
- **æ—¶é—´ä¿¡æ¯å®Œæ•´** - å‘å¸ƒã€æˆªæ­¢ã€å¼€å§‹ã€ç»“æŸæ—¥æœŸå…¨é¢æå–
- **è´¨é‡ä¿è¯** - æ•°æ®éªŒè¯ã€é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥
- **æ€§èƒ½ä¼˜åŒ–** - å¹¶å‘å¤„ç†ã€ç¼“å­˜æœºåˆ¶ã€é”™è¯¯æ¢å¤

### 7.3 é¢„æœŸæ•ˆæœ
- **æ ç›®è¯†åˆ«å‡†ç¡®ç‡** â‰¥ 90%
- **ä¿¡æ¯æå–å‡†ç¡®ç‡** â‰¥ 85%
- **å»é‡å‡†ç¡®ç‡** â‰¥ 95%
- **æ—¥æœŸæå–å‡†ç¡®ç‡** â‰¥ 90%
- **å¤„ç†é€Ÿåº¦** â‰¤ 30ç§’/åŒ»é™¢

è¯¥ç³»ç»Ÿä¸ºåŒ»é™¢æ‹›æŠ•æ ‡ç›‘æ§æä¾›äº†å¼ºå¤§çš„æŠ€æœ¯æ”¯æ’‘ï¼Œç¡®ä¿èƒ½å¤ŸåŠæ—¶ã€å‡†ç¡®åœ°å‘ç°å’Œè·Ÿè¸ªæ‹›æŠ•æ ‡ä¿¡æ¯çš„å˜åŒ–ï¼Œä¸ºä¸šåŠ¡å†³ç­–æä¾›å¯é çš„æ•°æ®åŸºç¡€ã€‚

**ä¸‹ä¸€æ­¥ï¼š** åŸºäºæ­¤è®¾è®¡æ–¹æ¡ˆï¼Œå®ç°å…·ä½“çš„ä»£ç æ¨¡å—å’Œé›†æˆæµ‹è¯•ã€‚